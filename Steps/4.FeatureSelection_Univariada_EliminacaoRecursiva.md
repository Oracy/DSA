# Feature Selection

• Os atributos presentes no seu dataset e que você utiliza nos dados de treino, terão grande influência na precisão e resultado do seu modelo preditivo. Atributos irrelevante terão impacto negativo na performance, enquanto atributos colineares podem afetar o grau de acurácia do modelo. O Scikit-learn possui funções que automatizam o trabalhao de extração e seleção de variáveis.

• A etapa de Feature Selection é onde selecionamos os atributos (variáveis) que serão melhores candidatas a variáveis preditoras. O Feature Selection nos ajuda a reduzir o overfitting (quando o algoritmo aprende demais), aumenta a acurácia do modelo e reduz o tempo de treinamento.


1. Seleção Univariada

• Testes estatísticos podem ser usados para selecionar os atributos que possuem forte relacionamento com a variável que estamos tentando prever. O Scikit-learn fornece a função SelectKBest() que pode ser usada com diversos testes estatísticos, para selecionar os atributos. Vamos usar um teste que estudamos no capítulo anterior, o teste qui-quadrado. Vamos seelcionar os 4 melhores atributos que podem ser usados como variáveis preditoras.

• Veremos abaixo o score para cada atributo e os 4 atributos com maior score e que portanto devem ser selecionados como variáveis preditoras.

```python
    # Extração de Variáveis com Testes Estatísticos Univariados (Teste qui-quadrado)

    # Import dos módulos
    from pandas import read_csv
    from sklearn.feature_selection import SelectKBest
    from sklearn.feature_selection import chi2

    # Carregando os dados
    url = "http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv"
    colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
    df = read_csv(url, names = colunas)
    array = df.values

    # Separando o array em componentes de input e output
    X = array[:,0:8]
    Y = array[:,8]

    # Extração de Variáveis
    test = SelectKBest(score_func = chi2, k = 4)
    fit = test.fit(X, Y)

    # Sumarizando o score
    print(fit.scores_)
    features = fit.transform(X)

    # Sumarizando atributos selecionados
    print(features[0:5,:])
```


# Eliminação Recursiva de Atributos

• Esta é outra técnica para seleção de atributos, que recursivamente remove os atributos e constrói o modelo com os atributos remanescentes. Esta técnica utiliza a acurácia do modelo para identificar os atributos que mais contribuem para prever a variável alvo. Em inglês esta técnia é chamada Recursive Feature Elimination (RFE).

• O exemplo abaixo utiliza a técnica de eliminação recursiva de atributos com um algoritmo de Regressão Logística para selecionar as 3 melhores variáveis preditoras. O RFE selecionou as variáveis preg, mass e pedi, que estão marcadas como True em "Atributos Selecionados" e com valor 1 em "Ranking dos Atributos".

```python
    # Eliminação Recursiva de Variáveis

    # Import dos módulos
    from pandas import read_csv
    from sklearn.feature_selection import RFE
    from sklearn.linear_model import LogisticRegression

    # Carregando os dados
    url = "http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv"
    colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
    df = read_csv(url, names = colunas)
    array = df.values

    # Separando o array em componentes de input e output
    X = array[:,0:8]
    Y = array[:,8]

    # Criação do modelo
    modelo = LogisticRegression()

    # RFE
    rfe = RFE(modelo, 3)
    fit = rfe.fit(X, Y)

    # Print dos resultados
    print("Número de Atributos: %d" % fit.n_features_)
    print(df.columns[0:8])
    print("Atributos Selecionados: %s" % fit.support_)
    print("Ranking dos Atributos: %s" % fit.ranking_)
```